==================== SFL ====================                                  
Experiment Arguments:                                                          
{
    'method': 'sfl',
    'dataset': {
        'name': 'cifar10',
        'client_num': 20,
        'test_ratio': 0.25,
        'val_ratio': 0.0,
        'seed': 42,
        'split': 'sample',
        'IID_ratio': 0.0,
        'monitor_window_name_suffix': 'cifar10-20clients-0%IID-Dir(0.5)-seed42',
        'alpha': 0.5,
        'min_samples_per_client': 10
    },
    'model': {
        'name': 'avgcnn',
        'use_torchvision_pretrained_weights': True,
        'external_model_weights_path': None
    },
    'optimizer': {
        'lr': 0.01,
        'dampening': 0,
        'weight_decay': 0,
        'momentum': 0,
        'nesterov': False,
        'name': 'sgd'
    },
    'mode': 'sequential',
    'parallel': {
        'ray_cluster_addr': None,
        'num_cpus': None,
        'num_gpus': None,
        'num_workers': 2
    },
    'common': {
        'seed': 42,
        'join_ratio': 0.2,
        'global_epoch': 100,
        'local_epoch': 5,
        'batch_size': 32,
        'reset_optimizer_on_global_epoch': True,
        'straggler_ratio': 0,
        'straggler_min_local_epoch': 0,
        'buffers': 'global',
        'client_side_evaluation': True,
        'test': {
            'client': {
                'interval': 50,
                'finetune_epoch': 0,
                'train': False,
                'val': False,
                'test': True
            },
            'server': {
                'interval': -1,
                'train': False,
                'val': False,
                'test': False,
                'model_in_train_mode': False
            }
        },
        'verbose_gap': 10,
        'monitor': None,
        'use_cuda': True,
        'save_log': True,
        'save_model': False,
        'save_learning_curve_plot': False,
        'save_metrics': True,
        'delete_useless_run': True
    }
}
---------------------------- TRAINING EPOCH: 10 ----------------------------   
client [12] (testset)   loss: 1.7573 -> 0.4760  accuracy: 54.92% -> 86.27%     
client [3]  (testset)   loss: 1.6411 -> 0.6509  accuracy: 62.03% -> 81.15%     
client [11] (testset)   loss: 3.2561 -> 0.8711  accuracy: 40.18% -> 73.64%     
client [17] (testset)   loss: 2.0985 -> 1.0123  accuracy: 49.38% -> 69.05%     
---------------------------- TRAINING EPOCH: 20 ----------------------------   
client [17] (testset)   loss: 5.5933 -> 1.2292  accuracy: 30.26% -> 71.39%     
client [7]  (testset)   loss: 1.9779 -> 1.7689  accuracy: 62.51% -> 68.21%     
client [10] (testset)   loss: 1.3752 -> 0.7286  accuracy: 75.53% -> 83.63%     
client [1]  (testset)   loss: 2.6407 -> 1.2412  accuracy: 56.76% -> 76.57%     
---------------------------- TRAINING EPOCH: 30 ----------------------------   
client [19] (testset)   loss: 2.2673 -> 1.3275  accuracy: 58.92% -> 72.07%     
client [14] (testset)   loss: 2.9070 -> 1.1493  accuracy: 47.24% -> 79.28%     
client [16] (testset)   loss: 4.1749 -> 0.8438  accuracy: 36.85% -> 81.39%     
client [8]  (testset)   loss: 2.0874 -> 0.6536  accuracy: 60.80% -> 87.19%     
---------------------------- TRAINING EPOCH: 40 ----------------------------   
client [15] (testset)   loss: 2.9375 -> 1.0602  accuracy: 53.58% -> 77.04%     
client [2]  (testset)   loss: 4.3367 -> 0.7981  accuracy: 47.74% -> 84.31%     
client [17] (testset)   loss: 3.6641 -> 1.4985  accuracy: 51.58% -> 70.84%     
client [4]  (testset)   loss: 1.9490 -> 1.4376  accuracy: 65.52% -> 73.56%     
---------------------------- TRAINING EPOCH: 50 ----------------------------   
client [8]  (testset)   loss: 3.9247 -> 0.8080  accuracy: 42.07% -> 86.23%     
client [15] (testset)   loss: 3.5718 -> 1.0979  accuracy: 53.95% -> 76.79%     
client [6]  (testset)   loss: 1.6731 -> 1.3032  accuracy: 71.96% -> 76.87%     
client [4]  (testset)   loss: 3.3234 -> 1.4233  accuracy: 50.57% -> 72.27%     
---------------------------- TRAINING EPOCH: 60 ----------------------------   
client [2]  (testset)   loss: 1.7374 -> 0.9837  accuracy: 70.99% -> 83.63%     
client [7]  (testset)   loss: 3.3008 -> 2.1207  accuracy: 57.41% -> 66.90%     
client [5]  (testset)   loss: 2.5047 -> 1.8067  accuracy: 60.62% -> 68.20%     
client [13] (testset)   loss: 1.7673 -> 1.2205  accuracy: 71.31% -> 79.00%     
---------------------------- TRAINING EPOCH: 70 ----------------------------   
client [7]  (testset)   loss: 2.4911 -> 2.2312  accuracy: 64.29% -> 69.16%     
client [12] (testset)   loss: 1.8595 -> 0.9521  accuracy: 74.75% -> 86.95%     
client [3]  (testset)   loss: 2.2782 -> 1.1059  accuracy: 73.14% -> 83.54%     
client [19] (testset)   loss: 3.0785 -> 1.7213  accuracy: 57.66% -> 72.79%     
---------------------------- TRAINING EPOCH: 80 ----------------------------   
client [17] (testset)   loss: 3.4552 -> 1.7804  accuracy: 56.40% -> 71.94%     
client [9]  (testset)   loss: 2.8998 -> 1.3376  accuracy: 62.76% -> 79.39%     
client [16] (testset)   loss: 2.2140 -> 1.0599  accuracy: 66.39% -> 82.44%     
client [0]  (testset)   loss: 2.7144 -> 1.6672  accuracy: 69.13% -> 75.76%     
---------------------------- TRAINING EPOCH: 90 ----------------------------   
client [11] (testset)   loss: 2.9057 -> 1.6251  accuracy: 64.73% -> 75.64%     
client [18] (testset)   loss: 3.3237 -> 1.2694  accuracy: 57.61% -> 80.20%     
client [17] (testset)   loss: 4.1735 -> 1.7245  accuracy: 57.08% -> 71.39%     
client [4]  (testset)   loss: 2.2323 -> 1.7299  accuracy: 64.80% -> 73.56%     
---------------------------- TRAINING EPOCH: 100 ----------------------------  
client [0]  (testset)   loss: 3.4561 -> 1.8348  accuracy: 61.36% -> 75.00%     
client [6]  (testset)   loss: 3.0674 -> 1.5403  accuracy: 62.02% -> 77.39%     
client [12] (testset)   loss: 3.0185 -> 1.0223  accuracy: 63.90% -> 86.95%     
client [10] (testset)   loss: 3.5135 -> 1.2003  accuracy: 59.32% -> 83.31%     
SFL's average time taken by each global epoch: 0 min 4.25 sec.                 
SFL's total running time: 0 h 7 m 7 s.                                         
==================== SFL Experiment Results: ====================              
Display format: (before local fine-tuning) -> (after local fine-tuning)        
 So if finetune_epoch = 0, x.xx% -> 0.00% is normal.                           
 Centralized testing ONLY happens after model aggregation, so the stats between
'->' are the same.                                                             
{                                                                              
    "50": {                                                                    
        "all_clients": {                                                       
            "test": {                                                          
                "loss": "2.7363 -> 0.0000",                                    
                "accuracy": "57.63% -> 0.00%"                                  
            }                                                                  
        }                                                                      
    },                                                                         
    "100": {                                                                   
        "all_clients": {                                                       
            "test": {                                                          
                "loss": "2.8750 -> 0.0000",                                    
                "accuracy": "61.98% -> 0.00%"                                  
            }                                                                  
        }                                                                      
    }                                                                          
}                                                                              
==================== SFL Max Accuracy ====================                     
all_clients:                                                                   
(test) before fine-tuning: 61.98% at epoch 100                                 
(test) after fine-tuning: 0.00% at epoch 50                                    
