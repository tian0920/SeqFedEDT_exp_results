==================== SFL ====================                                  
Experiment Arguments:                                                          
{
    'method': 'sfl',
    'dataset': {
        'name': 'cifar100',
        'client_num': 20,
        'test_ratio': 0.25,
        'val_ratio': 0.0,
        'seed': 42,
        'split': 'sample',
        'IID_ratio': 0.0,
        'monitor_window_name_suffix': 'cifar100-20clients-0%IID-use20superclasses-Dir(0.5)-seed42',
        'super_class': False,
        'alpha': 0.5,
        'min_samples_per_client': 10
    },
    'model': {
        'name': 'avgcnn',
        'use_torchvision_pretrained_weights': True,
        'external_model_weights_path': None
    },
    'optimizer': {
        'lr': 0.01,
        'dampening': 0,
        'weight_decay': 0,
        'momentum': 0,
        'nesterov': False,
        'name': 'sgd'
    },
    'mode': 'sequential',
    'parallel': {
        'ray_cluster_addr': None,
        'num_cpus': None,
        'num_gpus': None,
        'num_workers': 2
    },
    'common': {
        'seed': 42,
        'join_ratio': 0.2,
        'global_epoch': 100,
        'local_epoch': 5,
        'batch_size': 32,
        'reset_optimizer_on_global_epoch': True,
        'straggler_ratio': 0,
        'straggler_min_local_epoch': 0,
        'buffers': 'global',
        'client_side_evaluation': True,
        'test': {
            'client': {
                'interval': 50,
                'finetune_epoch': 0,
                'train': False,
                'val': False,
                'test': True
            },
            'server': {
                'interval': -1,
                'train': False,
                'val': False,
                'test': False,
                'model_in_train_mode': False
            }
        },
        'verbose_gap': 10,
        'monitor': None,
        'use_cuda': True,
        'save_log': True,
        'save_model': False,
        'save_learning_curve_plot': False,
        'save_metrics': True,
        'delete_useless_run': True
    }
}
---------------------------- TRAINING EPOCH: 10 ----------------------------   
client [12] (testset)   loss: 6.1636 -> 3.5735  accuracy: 13.65% -> 28.73%     
client [3]  (testset)   loss: 5.1702 -> 3.9500  accuracy: 16.03% -> 34.03%     
client [11] (testset)   loss: 6.2695 -> 3.4599  accuracy: 16.09% -> 34.56%     
client [17] (testset)   loss: 5.5913 -> 3.7088  accuracy: 18.14% -> 34.07%     
---------------------------- TRAINING EPOCH: 20 ----------------------------   
client [17] (testset)   loss: 7.1787 -> 4.6588  accuracy: 20.85% -> 35.93%     
client [7]  (testset)   loss: 6.8348 -> 5.2968  accuracy: 22.91% -> 33.08%     
client [10] (testset)   loss: 6.7675 -> 4.1340  accuracy: 21.62% -> 39.56%     
client [1]  (testset)   loss: 6.8425 -> 4.1430  accuracy: 17.93% -> 39.60%     
---------------------------- TRAINING EPOCH: 30 ----------------------------   
client [19] (testset)   loss: 6.9816 -> 4.7517  accuracy: 23.08% -> 34.97%     
client [14] (testset)   loss: 6.5214 -> 4.6564  accuracy: 23.29% -> 36.34%     
client [16] (testset)   loss: 6.4823 -> 4.1999  accuracy: 21.96% -> 35.91%     
client [8]  (testset)   loss: 6.4987 -> 5.0590  accuracy: 21.78% -> 36.05%     
---------------------------- TRAINING EPOCH: 40 ----------------------------   
client [15] (testset)   loss: 7.4411 -> 4.7421  accuracy: 22.49% -> 35.22%     
client [2]  (testset)   loss: 6.5984 -> 5.2754  accuracy: 22.77% -> 37.17%     
client [17] (testset)   loss: 7.2680 -> 5.4034  accuracy: 26.10% -> 38.31%     
client [4]  (testset)   loss: 6.6709 -> 4.6489  accuracy: 30.29% -> 42.31%     
---------------------------- TRAINING EPOCH: 50 ----------------------------   
client [8]  (testset)   loss: 7.8282 -> 6.1696  accuracy: 21.91% -> 33.89%     
client [15] (testset)   loss: 8.1279 -> 5.1052  accuracy: 23.88% -> 35.12%     
client [6]  (testset)   loss: 7.8019 -> 5.8867  accuracy: 20.23% -> 32.63%     
client [4]  (testset)   loss: 7.7025 -> 4.8801  accuracy: 25.11% -> 41.55%     
---------------------------- TRAINING EPOCH: 60 ----------------------------   
client [2]  (testset)   loss: 8.5648 -> 6.0786  accuracy: 23.82% -> 39.14%     
client [7]  (testset)   loss: 8.0403 -> 6.5923  accuracy: 24.43% -> 33.99%     
client [5]  (testset)   loss: 7.4670 -> 5.5968  accuracy: 28.13% -> 38.22%     
client [13] (testset)   loss: 7.3346 -> 5.6656  accuracy: 27.27% -> 35.05%     
---------------------------- TRAINING EPOCH: 70 ----------------------------   
client [7]  (testset)   loss: 8.2539 -> 7.0923  accuracy: 28.22% -> 33.54%     
client [12] (testset)   loss: 8.4904 -> 6.5287  accuracy: 23.49% -> 33.65%     
client [3]  (testset)   loss: 8.7107 -> 6.4200  accuracy: 22.86% -> 35.35%     
client [19] (testset)   loss: 8.6601 -> 6.2119  accuracy: 24.20% -> 35.38%     
---------------------------- TRAINING EPOCH: 80 ----------------------------   
client [17] (testset)   loss: 8.6605 -> 6.5326  accuracy: 25.76% -> 36.10%     
client [9]  (testset)   loss: 8.7308 -> 6.5834  accuracy: 26.11% -> 34.82%     
client [16] (testset)   loss: 7.2309 -> 5.9681  accuracy: 27.30% -> 37.69%     
client [0]  (testset)   loss: 7.9239 -> 5.8977  accuracy: 26.04% -> 39.90%     
---------------------------- TRAINING EPOCH: 90 ----------------------------   
client [11] (testset)   loss: 9.2307 -> 6.5474  accuracy: 21.64% -> 36.28%     
client [18] (testset)   loss: 8.9661 -> 6.5056  accuracy: 24.08% -> 35.49%     
client [17] (testset)   loss: 8.6292 -> 7.0848  accuracy: 23.56% -> 34.41%     
client [4]  (testset)   loss: 8.0946 -> 6.2644  accuracy: 29.22% -> 41.25%     
---------------------------- TRAINING EPOCH: 100 ----------------------------  
client [0]  (testset)   loss: 8.4086 -> 6.6528  accuracy: 28.21% -> 39.40%     
client [6]  (testset)   loss: 9.6963 -> 7.7329  accuracy: 21.37% -> 31.65%     
client [12] (testset)   loss: 8.6216 -> 7.2869  accuracy: 26.19% -> 33.97%     
client [10] (testset)   loss: 8.8849 -> 6.8083  accuracy: 26.97% -> 40.61%     
SFL's average time taken by each global epoch: 0 min 3.88 sec.                 
SFL's total running time: 0 h 6 m 29 s.                                        
==================== SFL Experiment Results: ====================              
Display format: (before local fine-tuning) -> (after local fine-tuning)        
 So if finetune_epoch = 0, x.xx% -> 0.00% is normal.                           
 Centralized testing ONLY happens after model aggregation, so the stats between
'->' are the same.                                                             
{                                                                              
    "50": {                                                                    
        "all_clients": {                                                       
            "test": {                                                          
                "loss": "7.1830 -> 0.0000",                                    
                "accuracy": "24.80% -> 0.00%"                                  
            }                                                                  
        }                                                                      
    },                                                                         
    "100": {                                                                   
        "all_clients": {                                                       
            "test": {                                                          
                "loss": "9.4453 -> 0.0000",                                    
                "accuracy": "26.17% -> 0.00%"                                  
            }                                                                  
        }                                                                      
    }                                                                          
}                                                                              
==================== SFL Max Accuracy ====================                     
all_clients:                                                                   
(test) before fine-tuning: 26.17% at epoch 100                                 
(test) after fine-tuning: 0.00% at epoch 50                                    
